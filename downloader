#!/usr/bin/python3
import requests
import argparse
import concurrent.futures
import logging
import json
import sys
import time
import os

import constants
import utils

def get_file_info(url):
    """
    If the given URL doesn't exist, this exits the program.
    Otherwise, returns a tuple with the length of the file, and a boolean indicating whether the file server supports partial requests respectively.
    """
    response = requests.head(url)

    utils.pprint(response.headers)

    if response.status_code >=400:
        logging.critical("Bad request. Could not access the file from the URL passed in.")
        sys.exit(1)

    ACCEPT_RANGES_HEADER = "Accept-Ranges"
    is_byte_range_supported = ACCEPT_RANGES_HEADER in response.headers and response.headers[
        ACCEPT_RANGES_HEADER] == "bytes"

    # We have to download this file in a single-threaded fashion if we cannot get the size
    if "Content-Length" not in response.headers:
        return 0, False
    else:
        return int(response.headers["Content-Length"]), is_byte_range_supported


def download_file_chunk(input):
    """
    Takes a filename, url, and byte range, and downloads a chunk of the file.
    """
    filename = input['filename']
    url = input['url']
    file_range = input['range']

    starting_bytes = int(file_range.split("-")[0])

    written_bytes = 0

    partial_filename = constants.DOWNLOAD_DIRECTORY + file_range + "-" + filename

    logging.debug(f"My range is: {file_range}")

    with requests.get(url,
                      stream=True,
                      headers={"Range": f"bytes={file_range}"}) as stream:
        with open(partial_filename, "wb+") as file:
            for chunk in stream.iter_content(chunk_size=constants.CHUNK_SIZE):
                file.write(chunk)
                written_bytes += len(chunk)

        return partial_filename


def download_file_parallel(url, n_threads, filename, ranges):
    """
    This downloads the file in `n_threads` parts concurrently.
    Each part is chunked up as well, in case the part is also too large to fit into memory. 
    """
    with concurrent.futures.ThreadPoolExecutor(
            max_workers=n_threads) as executor:
        inputs = [{
            'filename': filename,
            'url': url,
            'range': byte_range
        } for byte_range in ranges]

        logging.debug(json.dumps(list(inputs), indent=4, sort_keys=True))

        futures = executor.map(download_file_chunk, inputs)

        # Will contain the filenames of combined files
        return list(futures)


def download_file_single_threaded(url, filename):
    """
    Downloads a file using a single thread.
    """
    with requests.get(url, stream=True) as stream:
        with open(constants.DOWNLOAD_DIRECTORY + filename, "wb+") as file:
            for chunk in stream.iter_content(chunk_size=constants.CHUNK_SIZE):
                file.write(chunk)
        return filename


def serial_combine_parts(filenames, actual_filename):
    """
    Appends the contents of each file in filenames to the 0th file in a serial fashion.
    """
    start_time = time.time()

    first_filename = filenames[0]
    with open(first_filename, "ab") as combined_file:
        for filename in filenames[1:]:
            read_again = True
            with open(filename, 'rb') as partial:
                while read_again:
                    data = partial.read(constants.CHUNK_SIZE)
                    read_again = len(data) > 0
                    combined_file.write(data)
            absolute_path = os.path.abspath(filename)

            # We don't need partials any more.
            os.unlink(absolute_path)

    os.rename(first_filename, actual_filename)

    end_time = time.time()

    logging.info(f"Merging of files took {end_time - start_time} seconds.")


def logtree_combine_parts():
    """
    An O(log n) file merging algorithm, that can be parallelized.
    """
    raise NotImplementedError("Tree merging algorithm not implemented yet. Use the serial for now.")

    

def combine_parts(filenames, actual_filename, method='serial'):
    """
    Wrapper method to combine partial files using the given method.
    """
    if method == 'serial':
        serial_combine_parts(filenames, actual_filename)
    elif method == 'logtree':
        logtree_combine_parts(filenames)

def parse_and_get_arguments():
    """
    Initializes the parser, grabs arguments and returns them.
    """
    parser = argparse.ArgumentParser(
        description=
        'Download a file with nThreads number of threads. There will be a reasonable limit to the maximum value of this variable.'
    )
    parser.add_argument("url",
                        metavar="url",
                        type=str,
                        nargs=1,
                        action="store",
                        help="URL of file that will be downloaded")
    parser.add_argument('-c',
                        metavar='n_threads',
                        type=int,
                        nargs=1,
                        action="store",
                        required=True,
                        dest='n_threads',
                        help='an integer for the number of worker threads')
    return vars(parser.parse_args())


def download_file_and_get_time(url, filename, n_threads):
    """
    Manages the overall downloading of the file and returns the time taken for the download in seconds.
    """

    s = requests.Session()

    # Retry if the connection is dropped
    s.mount('http://', requests.adapters.HTTPAdapter(max_retries=constants.MAX_RETRIES))

    length, is_byte_range_supported = get_file_info(url)

    logging.info(f"File size: {length}")

    start_time = None

    if is_byte_range_supported and n_threads > 1:

        # Easier than to give some threads nothing
        if n_threads > length:
            n_threads = length

        # Only positive number of threads are allowed
        if n_threads <= 0:
            n_threads = 1

        logging.info(f"Downloading the file using {n_threads} threads.")
        ranges = utils.get_byte_ranges(length, n_threads)

        logging.debug(f"Byte ranges: {ranges}")

        start_time = time.time()

        filenames = download_file_parallel(url, n_threads, filename, ranges)

        logging.debug(f"Filenames to merge: {filenames}")

        combine_parts(filenames, filename)
    else:
        logging.info(f"Downloading file using a single thread.")

        start_time = time.time()

        download_file_single_threaded(url, filename)

    end_time = time.time()

    return end_time - start_time

def main():
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s-%(relativeCreated)1d-%(threadName)s-%(message)s')

    # Steps
    # 1. Check if file URL is valid.
    # 2. Get length of file.
    # 3. Use a function to break into ranges, based on n_threads
    # 4. Cap nThreads to reasonable number (defined in constants.py)
    # 5. Use multithreading / multiprocessing to download file parts.
    # 6. Finally, combine pairs of successive parts into a single file.
    # 7. Rename to original file's name based on URL.

    arguments = parse_and_get_arguments()

    # To prevent servers from thinking this is a DDOS attack
    n_threads = min(constants.MAX_THREADS, arguments["n_threads"][0])

    logging.info(f"Number of threads: {n_threads}")

    url = arguments["url"][0]

    filename = url.split("/")[-1]

    elapsed_time_in_seconds = download_file_and_get_time(url, filename, n_threads)

    logging.info(f"File downloaded successfully in {elapsed_time_in_seconds} seconds.")


if __name__ == "__main__":
    main()